{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 初始化log\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "\n",
    "if not os.path.exists('./log/'):\n",
    "    os.mkdir('./log/')\n",
    "fh = logging.FileHandler('./log/log.log', mode='a', encoding='utf-8')\n",
    "fh.setFormatter(formatter)\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\"))\n",
    "console.setLevel(logging.INFO)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(console)\n",
    "\n",
    "# 各项参数设置\n",
    "dataset = 'data/THUCnews'\n",
    "labels_name = [w.strip() for w in open(os.path.join(dataset, 'class.txt'), 'r', encoding='utf8').readlines()]\n",
    "pad_size = 32\n",
    "device = torch.device('cpu')  # cpu\n",
    "# device = torch.device('cuda')  # 设备gpu\n",
    "batch_size = 4\n",
    "bert_path = 'bert-base-chinese'\n",
    "num_classes = len(labels_name)\n",
    "weight_decay = 0.02\n",
    "num_epochs = 30\n",
    "learning_rate = 5e-5\n",
    "bert_lr_ratio = 0.2\n",
    "dropout = 0.1\n",
    "patience = 6\n",
    "save_path = 'saved_dict/bert.ckpt' # 模型保存地址\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28000it [00:00, 925216.55it/s]\n",
      "7000it [00:00, 957041.79it/s]\n",
      "7000it [00:00, 1034280.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('中国春节推涨全球黄金市场', '13'), ('韩网游企业纷纷进军Facebook游戏领域', '9'), ('跟降热盘 新里西斯莱公馆精装3-4居19000起', '4'), ('组图：《网球王子2》上海开机 张德培加盟演出', '1'), ('资金爆炒 郑糖开始宽幅振荡', '13')]\n",
      "['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# 读取数据集\n",
    "def data_loader(file_path):\n",
    "    contents = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            content, label = lin.split('\\t')\n",
    "            contents.append((content, label))\n",
    "    return contents\n",
    "\n",
    "\n",
    "train_data = data_loader(os.path.join(dataset, 'train.txt'))\n",
    "dev_data = data_loader(os.path.join(dataset, 'dev.txt'))\n",
    "test_data = data_loader(os.path.join(dataset, 'test.txt'))\n",
    "\n",
    "print(test_data[:5])\n",
    "print(labels_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28000 [00:00<?, ?it/s]/Users/admin/opt/anaconda3/envs/dl/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 28000/28000 [00:03<00:00, 7685.28it/s]\n",
      "100%|██████████| 7000/7000 [00:00<00:00, 7636.90it/s]\n",
      "100%|██████████| 7000/7000 [00:01<00:00, 5916.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 8170, 7770, 5440, 2562, 2703, 1856, 2845, 8038, 6237, 6438, 2218, 689, 3297, 1391, 7676, 4638, 758, 1920, 683, 689, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel,AutoModel,AutoTokenizer\n",
    "\n",
    "# 数据encode\n",
    "def encode_data(tokenizer, data):\n",
    "    data_encoded = []\n",
    "    for text, label in tqdm(data, total=len(data)):\n",
    "        inputs = tokenizer.encode(text=text, max_length=pad_size, truncation=True,\n",
    "                                  truncation_strategy='longest_first',\n",
    "                                  add_special_tokens=True, pad_to_max_length=True)\n",
    "        data_encoded.append((inputs, [1 if x != 0 else 0 for x in inputs], int(label)))\n",
    "    return data_encoded\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "train_data = encode_data(tokenizer, train_data)\n",
    "dev_data = encode_data(tokenizer, dev_data)\n",
    "test_data = encode_data(tokenizer, test_data)\n",
    "\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101,  704, 1744, 3217, 5688, 2972, 3885, 1059, 4413, 7942, 7032, 2356,\n",
      "         1767,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 7506, 5381, 3952,  821,  689, 5290, 5290, 6822, 1092,  100, 3952,\n",
      "         2767, 7566, 1818,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6656, 7360, 4178, 4669, 3173, 7027, 6205, 3172, 5812, 1062, 7667,\n",
      "         5125, 6163,  124,  118,  125, 2233, 8985, 8129, 6629,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5299, 1745, 8038,  517, 5381, 4413, 4374, 2094,  123,  518,  677,\n",
      "         3862, 2458, 3322, 2476, 2548, 1824, 1217, 4673, 4028, 1139,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([13,  9,  4,  1])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "# 迭代器包装\n",
    "train_dataset = TensorDataset(*[torch.LongTensor(x).to(device) for x in zip(*train_data)])\n",
    "dev_dataset = TensorDataset(*[torch.LongTensor(x).to(device) for x in zip(*dev_data)])\n",
    "test_dataset = TensorDataset(*[torch.LongTensor(x).to(device) for x in zip(*test_data)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_iter = iter(test_loader)\n",
    "print(next(test_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=14, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 模型网络搭建\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, bert_path, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path) # 加载预训练模型\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True # 在训练中更新bert预训练模型的权重\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) # 全连接层分类\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[1]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, attention_mask=mask, return_dict=False)\n",
    "        out = self.fc(pooled)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(bert_path, 768, num_classes).to(device)\n",
    "print(model.parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/dl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "def get_optimizer(model):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    bert_param_ids = list(map(id, param_optimizer))\n",
    "    no_weight_decay_params = [x[1] for x in filter(\n",
    "        lambda name_w: any(nwd in name_w[0] for nwd in no_decay), model.named_parameters())]\n",
    "    no_weight_decay_param_ids = list(map(id, [x[1] for x in no_weight_decay_params]))\n",
    "    bert_base_params = filter(lambda p: id(p) in bert_param_ids and id(p) not in no_weight_decay_param_ids,\n",
    "                              model.parameters())\n",
    "    bert_no_weight_decay_params = filter(lambda p: id(p) in bert_param_ids and id(p) in no_weight_decay_param_ids,\n",
    "                                         model.parameters())\n",
    "    base_no_weight_decay_params = filter(\n",
    "        lambda p: id(p) not in bert_param_ids and id(p) in no_weight_decay_param_ids,\n",
    "        model.parameters())\n",
    "    base_params = filter(lambda p: id(p) not in bert_param_ids and id(p) not in no_weight_decay_param_ids,\n",
    "                         model.parameters())\n",
    "    params = [{\"params\": bert_base_params, \"lr\": learning_rate * bert_lr_ratio},\n",
    "              {\"params\": bert_no_weight_decay_params, \"lr\": learning_rate * bert_lr_ratio,\n",
    "               \"weight_decay\": 0.0},\n",
    "              {\"params\": base_no_weight_decay_params, \"lr\": learning_rate, \"weight_decay\": 0.0},\n",
    "              {\"params\": base_params, \"lr\": learning_rate}]\n",
    "\n",
    "    # 设置AdamW优化器\n",
    "    optimizer = AdamW(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "optimizer = get_optimizer(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 14:21:29,966 - INFO: Epoch [1/30]\n",
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 100\u001B[0m\n\u001B[1;32m     95\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     97\u001B[0m     test(model, test_iter)\n\u001B[0;32m--> 100\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdev_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[29], line 65\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_iter, dev_iter, test_iter, optimizer)\u001B[0m\n\u001B[1;32m     63\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(outputs, batch[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     64\u001B[0m train_loss_list\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m---> 65\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1.0\u001B[39m)  \u001B[38;5;66;03m# 梯度裁剪\u001B[39;00m\n\u001B[1;32m     67\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/dl/lib/python3.8/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/dl/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in tqdm(data_iter):\n",
    "            batch = [x.to(device) for x in batch]\n",
    "            outputs = model((batch[0], batch[1]))\n",
    "            # print(labels)\n",
    "            loss = F.cross_entropy(outputs, batch[-1])\n",
    "            loss_total += loss\n",
    "            labels = batch[-1].data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    f1 = f1_score(labels_all, predict_all, average='macro')\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=labels_name, digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "\n",
    "    return acc, f1, loss_total / (len(data_iter) + 1e-10), report, confusion\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    test_acc, test_f1, test_loss, test_report, test_confusion = evaluate(model, test_iter)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}, Test F1:{2:>6.2%}'\n",
    "    logger.info(msg.format(test_loss, test_acc, test_f1))\n",
    "    logger.info(\"Precision, Recall and F1-Score...\")\n",
    "    logger.info(test_report)\n",
    "    logger.info(\"Confusion Matrix...\")\n",
    "    logger.info(test_confusion)\n",
    "\n",
    "\n",
    "def train(model, train_iter, dev_iter, test_iter, optimizer):\n",
    "    model.train()\n",
    "    dev_best_f1 = float('-inf')\n",
    "    last_improve_epoch = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info('Epoch [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        # 记录变量\n",
    "        train_labels_all = np.array([], dtype=int)\n",
    "        train_predicts_all = np.array([], dtype=int)\n",
    "        train_loss_list = []\n",
    "        t = tqdm(train_iter, leave=False, total=len(train_iter), desc='Training')\n",
    "        for step, batch in enumerate(t):\n",
    "            batch = [x.to(device) for x in batch]\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            outputs = model((batch[0], batch[1]))\n",
    "            loss = F.cross_entropy(outputs, batch[-1])\n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 梯度裁剪\n",
    "            optimizer.step()\n",
    "            # 真实标签和预测标签\n",
    "\n",
    "            predicts = torch.max(outputs.data, 1)[1].cpu()\n",
    "            labels_train = batch[-1].cpu().data.numpy()\n",
    "            train_labels_all = np.append(train_labels_all, labels_train)\n",
    "            train_predicts_all = np.append(train_predicts_all, predicts)\n",
    "\n",
    "        # 训练集评估\n",
    "        train_loss = sum(train_loss_list) / (len(train_loss_list) + 1e-10)\n",
    "        train_acc = metrics.accuracy_score(train_labels_all, train_predicts_all)\n",
    "        train_f1 = metrics.f1_score(train_labels_all, train_predicts_all, average='macro')\n",
    "\n",
    "        dev_acc, dev_f1, dev_loss, report, confusion = evaluate(model, dev_iter)\n",
    "        msg = 'Train Loss: {0:>5.6},  Train Acc: {1:>6.4%},  Train F1: {2:>6.4%},  Val Loss: {3:>5.4},  Val Acc: {4:>6.4%},  Val F1: {5:>6.4%}'\n",
    "        logger.info(msg.format(train_loss, train_acc, train_f1, dev_loss, dev_acc, dev_f1))\n",
    "        logger.info(\"Precision, Recall and F1-Score...\")\n",
    "        logger.info(report)\n",
    "        logger.info(\"Confusion Matrix...\")\n",
    "        logger.info(confusion)\n",
    "\n",
    "        if dev_f1 > dev_best_f1:\n",
    "            dev_best_f1 = dev_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            last_improve_epoch = epoch\n",
    "\n",
    "        if epoch - last_improve_epoch > patience:\n",
    "            logger.info(\"No optimization for a long time, auto-stopping...\")\n",
    "            break\n",
    "\n",
    "    test(model, test_iter)\n",
    "\n",
    "\n",
    "train(model,train_loader,dev_loader,test_loader,optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
